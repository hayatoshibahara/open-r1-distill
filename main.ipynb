{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db3b17e",
   "metadata": {},
   "source": [
    "# Open-R1-Distill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c646831",
   "metadata": {},
   "source": [
    "- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning][3]\n",
    "- [huggingface/open-r1][2]\n",
    "- [open-r1/Mixture-of-Thoughts][1]\n",
    "\n",
    "[1]: https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts\n",
    "[2]: https://github.com/huggingface/open-r1\n",
    "[3]: https://arxiv.org/abs/2501.12948"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c178a5",
   "metadata": {},
   "source": [
    "## æ¦‚è¦\n",
    "\n",
    "è«–æ–‡ã§ã¯ã€DeepSeek-R1-Zeroã¨DeepSeek-R1ã‚’ç´¹ä»‹\n",
    "\n",
    "- DeepSeek-R1-Zeroã¯ã€æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã‚ãšã«å¤§è¦æ¨¡ãªå¼·åŒ–å­¦ç¿’ã‚’DeepSeek-V3-Baseã«é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«\n",
    "- DeepSeek-R1ã¯ã€DeepSeek-R1-Zeroã‚’æ”¹å–„ã—ãŸãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯DeepSeek-R1ã¨ã€ãã®æ¨è«–èƒ½åŠ›ã‚’è’¸ç•™ã—ãŸDeep-Seek-R1-distillã‚’è§£èª¬ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10053c15",
   "metadata": {},
   "source": [
    "DeepSeek-R1-Zeroã¯é©šç•°çš„ãªæ¨è«–èƒ½åŠ›ã‚’ç²å¾—ã—ãŸãŒã€ã€Œç”Ÿæˆã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®å¯èª­æ€§ã®ä½ã•ã€ã‚„ã€Œè¨€èªæ··åœ¨ã€ã®èª²é¡ŒãŒã‚ã£ãŸ\n",
    "\n",
    "DeepSeek-R1ã¯ã€å¼·åŒ–å­¦ç¿’ã®å‰ã«ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸è¨“ç·´ã¨ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå…¥ã‚ŒãŸ:\n",
    "\n",
    "- ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆcold-start dataï¼‰\n",
    "    - ç†æƒ³çš„ãªå›ç­”ã®å½¢å¼ã‚„æ€è€ƒéç¨‹ï¼ˆCoTï¼‰ã‚’æ•™ãˆã‚‹æ•°åƒä»¶ã®é«˜å“è³ªãƒ‡ãƒ¼ã‚¿\n",
    "    - CoTï¼ˆChain-of-Thoughtï¼‰ï¼šãƒ¢ãƒ‡ãƒ«ãŒå›ç­”ã‚’å‡ºã™ã¾ã§ã®æ€è€ƒã®éç¨‹ã§ã€OpenAI-o1ãŒç™ºç«¯\n",
    "- ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸è¨“ç·´ï¼ˆmulti-stage trainingï¼‰\n",
    "    1. ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸæ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆCold Startï¼‰\n",
    "    2. DeepSeek-R1-Zeroã¨åŒæ§˜ã®å¼·åŒ–å­¦ç¿’ï¼ˆReasoning-oriented RLï¼‰\n",
    "    3. ãƒªã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆRejection Samplingã¨SFTï¼‰\n",
    "        - ãƒªã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šãƒ¢ãƒ‡ãƒ«ã«å¤šæ•°ã®å›ç­”ã‚’ç”Ÿæˆã•ã›ã€æ­£ã—ã„å›ç­”ã®ã¿ã‚’SFTã«ä½¿ç”¨ã™ã‚‹æ‰‹æ³•\n",
    "        - äº‹å®Ÿã«åŸºã¥ãè³ªå•å¿œç­”ï¼ˆfactual QAï¼‰ã¨è‡ªå·±èªè­˜ï¼ˆself-cognitionï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›´ã«æ··ãœã¦SFT\n",
    "    4. å…¨ã‚·ãƒŠãƒªã‚ªã«å¯¾ã™ã‚‹å¼·åŒ–å­¦ç¿’ï¼ˆRF for Scenariosï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7970cdd3",
   "metadata": {},
   "source": [
    "DeepSeek-R1ï¼ˆ671Bï¼‰ã¯ã€OpenAI-o1ã«ç«¶åˆã™ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033d518",
   "metadata": {},
   "source": [
    "![](image/fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be80608",
   "metadata": {},
   "source": [
    "- AMIE 2024ï¼ˆAmerican Invitational Mathmatics Examinationsï¼‰: é«˜æ ¡ç”Ÿå‘ã‘æ•°å­¦é¸æŠœè©¦é¨“\n",
    "- Codeforcesï¼šç«¶æŠ€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ \n",
    "- GPQA Diamondï¼ˆGraduate-Level Google-Proof Q&A Benchmarkï¼‰ï¼šå¤§å­¦é™¢ãƒ¬ãƒ™ãƒ«ã®é›£å•ã‚’é›†ã‚ãŸå•é¡Œ\n",
    "- MATH-500ï¼šç«¶æŠ€æ•°å­¦ã®å•é¡Œã‚’é›†ã‚ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "- MMLUï¼ˆMassive Multitask Language Understandingï¼‰ï¼šæ­´å²ãƒ»åŒ»å­¦ãƒ»æ³•å¾‹ãƒ»æ•°å­¦ãªã©ã®57åˆ†é‡ã®å¤šè‚¢é¸æŠ\n",
    "- SWE-bench Verifiedï¼šã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb449536",
   "metadata": {},
   "source": [
    "è¨“ç·´æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹:\n",
    "\n",
    "- DeepSeek-R1-Zero\n",
    "- DeepSeek-R1\n",
    "- DeepSeek-R1ã‹ã‚‰å¾—ãŸçŸ¥è­˜ã‚’QwenãŠã‚ˆã³Llamaã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«è’¸ç•™ã—ãŸ6ã¤ã®denseãƒ¢ãƒ‡ãƒ«\n",
    "    - 1.5B\n",
    "    - 7B\n",
    "    - 14B\n",
    "    - 32B\n",
    "    - 70B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd41cff",
   "metadata": {},
   "source": [
    "DeepSeek-R1ã®ç›´æ¥çš„ãªè’¸ç•™ãŒã€å„ªã‚ŒãŸæ¨è«–èƒ½åŠ›ã‚’å¼•ãå‡ºã™ã“ã¨ã‚’ç™ºè¦‹\n",
    "\n",
    "- ã‚ˆã‚Šå¤§ããªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆDeepSeek-V3-Baseï¼‰ã®å¼·åŒ–å­¦ç¿’ã«ã‚ˆã£ã¦åŸ¹ã£ãŸæ€è€ƒéç¨‹ãŒæ¥µã‚ã¦é‡è¦\n",
    "- è’¸ç•™ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯ã€denseãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§é£›èºçš„ã«é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ãŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66b41d",
   "metadata": {},
   "source": [
    "## æ‰‹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78786f13",
   "metadata": {},
   "source": [
    "å¼·åŒ–å­¦ç¿’ã¯ã€GRPOï¼ˆGroup Relative Policy Optimizationï¼‰ã‚’æ¡ç”¨\n",
    "\n",
    "- å¼·åŒ–å­¦ç¿’ã§ä¸€èˆ¬çš„ãªã€Œãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã‚’è©•ä¾¡ã™ã‚‹ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã€ã‚’ä½¿ç”¨ã—ãªã„\n",
    "    1. æ–¹ç­–ãƒ¢ãƒ‡ãƒ«ï¼ˆpolicy modelï¼‰ã«å›ç­”ã‚’ç”Ÿæˆã•ã›ã‚‹\n",
    "    2. ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ï¼ˆcritic modelï¼‰ãŒå›ç­”ã‚’è©•ä¾¡ã—ã€å ±é…¬ã‚’ä¸ãˆã‚‹\n",
    "- ã‚°ãƒ«ãƒ¼ãƒ—ã‚¹ã‚³ã‚¢ã‹ã‚‰ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆåŸºæº–å€¤ï¼‰ã‚’æ¨å®šã™ã‚‹\n",
    "    1. ãƒ¢ãƒ‡ãƒ«ã«è¤‡æ•°ã®å›ç­”ã‚’ç”Ÿæˆã•ã›ã‚‹\n",
    "    2. å›ç­”ã‚’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§è‰²ã€…ãªè¦³ç‚¹ã‹ã‚‰è©•ä¾¡ã—ã€å ±é…¬ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—ã‚¹ã‚³ã‚¢ï¼‰\n",
    "        - æ•°å¼ã®å›ç­”ãŒã‚ã£ã¦ã„ã‚‹ã‹ï¼Ÿ\n",
    "        - å›ç­”ãŒçŸ­ã™ããªã„ã‹ï¼Ÿ\n",
    "        - ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒå®Ÿéš›ã«å‹•ãã‹ï¼Ÿ\n",
    "        - å›ç­”ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æ²¿ã£ã¦ã„ã‚‹ã‹ï¼Ÿ\n",
    "    3. ã‚°ãƒ«ãƒ¼ãƒ—ã®ä¸­ã§ç›¸å¯¾çš„ã«é«˜ã„å›ç­”ã‚’ã‚ˆã‚Šå¤šãç”Ÿæˆã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹ï¼ˆé€†ã‚‚ã—ã‹ã‚Šï¼‰\n",
    "- ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã‚ãªã„ã®ã§åŠ¹ç‡çš„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15345e53",
   "metadata": {},
   "source": [
    "GRPOã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :\n",
    "\n",
    "$$\n",
    "\\mathcal{J}_{GRPO}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)] \\\\\n",
    "\\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)}, 1-\\epsilon, 1+\\epsilon \\right) A_i \\right) - \\beta \\mathbb{D}_{KL}(\\pi_{\\theta} || \\pi_{ref}) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4c378",
   "metadata": {},
   "source": [
    "- $\\mathcal{J}_{GRPO}(\\theta)$: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\\theta$ã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã®ç›®çš„é–¢æ•°\n",
    "- $\\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)]$\n",
    "    - $\\pi_{\\theta_{old}}(O|q)$: å¤ã„ãƒ¢ãƒ‡ãƒ«ãŒè³ªå•$q$ã«å¯¾ã—ã¦å›ç­”$O$ã‚’ç”Ÿæˆã™ã‚‹ç¢ºç‡\n",
    "    - ä»¥ä¸‹ã®å ´åˆã§æå¤±ã‚’è¨ˆç®—ã—ãŸã¨ãã®å¹³å‡\n",
    "        - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿$P(Q)$ã‹ã‚‰è³ªå•$q$ã‚’é¸ã¶\n",
    "        - æ›´æ–°å‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦$G$å€‹ã®å›ç­”$o_i$ã‚’ç”Ÿæˆã™ã‚‹\n",
    "- $\\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)}$: å¤ã„ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦ã€æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ãŒãã®å›ç­”$o_i$ã‚’ç”Ÿæˆã™ã‚‹æ¯”ç‡\n",
    "- $A_i$: å›ç­”$o_i$ãŒã€ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®ä»–ã®è§£ç­”ã¨æ¯”ã¹ã¦ã©ã®ãã‚‰ã„è‰¯ã‹ã£ãŸã‹ï¼ˆã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰\n",
    "- $\\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)} A_i$:\n",
    "    - ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ãŒãƒ—ãƒ©ã‚¹ã®ã¨ãã€ãã®å›ç­”ã®ç”Ÿæˆç¢ºç‡ã‚’å¢—ã‚„ã™\n",
    "    - ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ãŒãƒã‚¤ãƒŠã‚¹ã®ã¨ãã€ãã®å›ç­”ã®ç”Ÿæˆç¢ºç‡ã‚’æ¸›ã‚‰ã™\n",
    "- $\\min \\left(..., \\text{clip} \\left( \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)}, 1-\\epsilon, 1+\\epsilon \\right) A_i \\right)$\n",
    "    - $\\epsilon$: ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ä¿‚æ•°\n",
    "    - å­¦ç¿’ãŒæ€¥æ¿€ã«é€²ã¿ã™ããªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°\n",
    "- $- \\beta \\mathbb{D}_{KL}(\\pi_{\\theta} || \\pi_{ref})$\n",
    "    - å…ƒã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¤‰åŒ–ã—ã™ããªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®é …ï¼ˆè‡ªç„¶ãªæ–‡ç« ãªã©ã®åŸºæœ¬èƒ½åŠ›ã‚’è½ã¨ã•ãªã„ï¼‰\n",
    "    - $\\mathbb{D}_{KL}$: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆäºŒã¤ã®ãƒ¢ãƒ‡ãƒ«ã®è·é›¢ï¼‰\n",
    "- $\\frac{1}{G} \\sum_{i=1}^G$: Gå€‹ã®å›ç­”ã®ã™ã¹ã¦ã®å¹³å‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be29a4",
   "metadata": {},
   "source": [
    "KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®è¿‘ä¼¼:\n",
    "\n",
    "$$\n",
    "\\mathbb{D}_{KL}(\\pi_{\\theta} || \\pi_{ref}) = \\frac{\\pi_{ref}(o_i|q)}{\\pi_{\\theta}(o_i|q)} - \\log \\frac{\\pi_{ref}(o_i|q)}{\\pi_{\\theta}(o_i|q)} - 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13532e",
   "metadata": {},
   "source": [
    "- ç¢ºç‡ã®æ¯”ç‡ã‚’å†åˆ©ç”¨ã™ã‚‹ã“ã¨ã§è¨ˆç®—ã‚’åŠ¹ç‡åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc65b86",
   "metadata": {},
   "source": [
    "ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸é–¢æ•°:\n",
    "\n",
    "$$\n",
    "A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\dots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\dots, r_G\\})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15274e",
   "metadata": {},
   "source": [
    "- å›ç­”$o_i$ãŒã‚°ãƒ«ãƒ¼ãƒ—ã®ä¸­ã§ã©ã®ç¨‹åº¦ã€å„ªã‚Œã¦ã„ãŸã‹ã®å€¤ï¼ˆ$r_i$ï¼‰ã‚’æ­£è¦åŒ–ã—ãŸã‚¹ã‚³ã‚¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2940372",
   "metadata": {},
   "source": [
    "## DeepSeek-R1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ba108",
   "metadata": {},
   "source": [
    "DeepSeek-R1ã§ã¯ã€å¼·åŒ–å­¦ç¿’ã®å‰ã«ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸ\n",
    "\n",
    "ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ©ç‚¹:\n",
    "\n",
    "1. å¯èª­æ€§ï¼ˆreadablityï¼‰\n",
    "    - å„å¿œç­”ã®æœ«å°¾ã«è¦ç´„ã‚’å«ã‚ã€èª­ã¿ã«ãã„å¿œç­”ã‚’é™¤å¤–ã™ã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ç”¨\n",
    "    - `|special_token| <reasoning_process> | special_token | <summary>`\n",
    "        - `<reasoning_process>`: æ€è€ƒã®éç¨‹\n",
    "        - `<summary>`ï¼šè¦ç´„\n",
    "2. å¯èƒ½æ€§ï¼ˆpotentialï¼‰\n",
    "    - äººé–“ã®ç”Ÿå¾—çš„ãªçŸ¥è­˜ï¼ˆhuman priorsï¼‰ã«åŸºã¥ã„ã¦è¨­è¨ˆï¼ˆè©³ã—ãã¯æ›¸ã‹ã‚Œã¦ã„ãªã„ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c2007",
   "metadata": {},
   "source": [
    "ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®SFTå¾Œã¯ã€DeepSeek-R1-Zeroã¨åŒæ§˜ã®å¤§è¦æ¨¡ãªå¼·åŒ–å­¦ç¿’ã‚’é©ç”¨:\n",
    "\n",
    "- ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ»æ•°å­¦ãƒ»ç§‘å­¦ãƒ»è«–ç†çš„æ¨è«–ãªã©ã€æ˜ç¢ºãªå›ç­”ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯ãŒä¸­å¿ƒ\n",
    "- è¨€èªã®æ··åœ¨ï¼ˆlanguage mixingï¼‰ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã«ã€è¨€èªçš„ä¸€è²«æ€§å ±é…¬ã‚’å°å…¥ï¼ˆlanguage consistency rewardï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43668f",
   "metadata": {},
   "source": [
    "å¼·åŒ–å­¦ç¿’ã®åæŸå¾Œã€ãƒªã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå¤šæ•°ç”Ÿæˆã—ã¦è‰¯ã„ã‚‚ã®ã‚’é¸ã¶åˆæˆæ‰‹æ³•ï¼‰ã¨2å›ç›®ã®SFTã‚’å®Ÿæ–½\n",
    "\n",
    "ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ãƒ»ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ³ã‚°ãƒ»ãã®ä»–ã®æ±ç”¨ã‚¿ã‚¹ã‚¯ã®èƒ½åŠ›ã®å‘ä¸ŠãŒç›®çš„ã§ã€ä»¥ä¸‹ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ:\n",
    "\n",
    "- æ€è€ƒãƒ‡ãƒ¼ã‚¿ï¼ˆreasoning dataï¼‰\n",
    "    - ãƒªã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã«è¤‡æ•°ã®å›ç­”ã‚’ç”Ÿæˆã—ã€è‰¯ã„å›ç­”ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ç”¨ã„ã‚‹\n",
    "    - æ˜ç¢ºãªå›ç­”ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯ä»¥å¤–ã«ã‚‚å¯¾å¿œã™ã‚‹ãŸã‚ã€DeepSeek-V3ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’è©•ä¾¡ï¼ˆgenerative reward modelï¼‰\n",
    "    - è¨€èªãŒæ··åœ¨ã—ã¦ã„ã‚‹CoTã€é•·ã„æ®µè½ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å«ã‚€å›ç­”ã¯å‰Šé™¤ï¼ˆãƒªã‚¸ã‚§ã‚¯ãƒˆï¼‰\n",
    "    - åˆè¨ˆ60ä¸‡ä»¶\n",
    "- éæ€è€ƒãƒ‡ãƒ¼ã‚¿ï¼ˆnon-reasoning dataï¼‰\n",
    "    - äº‹å®Ÿã«åŸºã¥ãè³ªç–‘å¿œç­”ï¼ˆfactual QAï¼‰ã€è‡ªå·±èªè­˜ï¼ˆself-cofnitionï¼‰ã€ç¿»è¨³ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿\n",
    "    - DeepSeek-V3ã®SFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸€éƒ¨ã‚’å†åˆ©ç”¨\n",
    "    - ç‰¹å®šã®éæ¨è«–ã‚¿ã‚¹ã‚¯ã¯ã€DeepSeek-V3ã§CoTã‚’ç”Ÿæˆ\n",
    "    - åˆè¨ˆ20ä¸‡ä»¶\n",
    "- åˆè¨ˆ80ä¸‡ä»¶ã‚’2ã‚¨ãƒãƒƒã‚¯ã§SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b742d",
   "metadata": {},
   "source": [
    "2å›ç›®ã®SFTå¾Œã¯ã€å…¨ã‚·ãƒŠãƒªã‚ªã«å¯¾ã™ã‚‹å¼·åŒ–å­¦ç¿’ã‚’å®Ÿæ–½ï¼ˆReinforcement Learning for all Scenariosï¼‰\n",
    "\n",
    "ç›®çš„ã¯2ã¤:\n",
    "\n",
    "- ãƒ¢ãƒ‡ãƒ«ã®æœ‰ç”¨æ€§ï¼ˆHelpfulnessï¼‰ã¨ç„¡å®³æ€§ï¼ˆHarmlessnessï¼‰ã®å‘ä¸Š\n",
    "- æ¨è«–èƒ½åŠ›ã‚’æ›´ã«æ´—ç·´ã•ã›ã‚‹ï¼ˆå¤šæ§˜ãªåˆ†é‡ã‚„ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ï¼‰\n",
    "\n",
    "è¨“ç·´:\n",
    "\n",
    "- æ€è€ƒãƒ‡ãƒ¼ã‚¿ã¯ã€æ•°å­¦ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ»è«–ç†æ¨è«–ã«å¯¾å¿œå¯èƒ½ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®å ±é…¬ã‚’åˆ©ç”¨\n",
    "- éæ€è€ƒãƒ‡ãƒ¼ã‚¿ã¯ã€è¤‡é›‘ãªäººé–“ã®å¥½ã¿ã‚’æ‰ãˆã‚‹ãŸã‚ã«å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
    "- æœ‰ç”¨æ€§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã®è¦ç´„ã«ç„¦ç‚¹ã‚’å……ã¦æ”¹å–„\n",
    "- ç„¡å®³æ€§ã¯ã€æ€è€ƒéç¨‹ã¨è¦ç´„ä¸¡æ–¹ã‚’è©•ä¾¡ã—ã€æœ‰å®³ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç‰¹å®šã—ã¦è»½æ¸›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aeb501",
   "metadata": {},
   "source": [
    "## è’¸ç•™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a55bb",
   "metadata": {},
   "source": [
    "DeepSeek-R1ã®èƒ½åŠ›ã‚’ã‚ˆã‚Šå°ã•ãªãƒ¢ãƒ‡ãƒ«ã§å†ç¾ã™ã‚‹ãŸã‚ã«ã€è’¸ç•™ï¼ˆDistillationï¼‰ã‚’å®Ÿæ–½:\n",
    "\n",
    "- DeepSeek-R1ã§ç”Ÿæˆã—ãŸ80ä¸‡ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰\n",
    "- Qwenã‚„Llamaãªã©ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- å¾Œç¶šã®ç ”ç©¶ã®ãŸã‚ã«ã€å¼·åŒ–å­¦ç¿’ã¯çµ„ã¿è¾¼ã¾ãªã‹ã£ãŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c519a7b",
   "metadata": {},
   "source": [
    "## è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc7e2a",
   "metadata": {},
   "source": [
    "[simple-evals][1]ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a32aa",
   "metadata": {},
   "source": [
    "ç§‘å­¦ã‚„æ•°å­¦ã®å•é¡Œã§V3ã‚ˆã‚Šã‚‚ã‹ãªã‚Šå¼·åŒ–ã•ã‚ŒãŸãŒã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦ï¼ˆAiderãªã©ï¼‰ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ãŸã‚å¼±ã„:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e932b03",
   "metadata": {},
   "source": [
    "![](image/table4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a093c92b",
   "metadata": {},
   "source": [
    "DeepSeek-R1ã®å‡ºåŠ›ã‚’è’¸ç•™ã•ã›ã‚‹ã ã‘ã§ã€å¼·åŠ›ãªæ€§èƒ½ã‚’ç¤ºã—ãŸ:\n",
    "\n",
    "![](image/table5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f476ae9",
   "metadata": {},
   "source": [
    "å°ã•ãªãƒ¢ãƒ‡ãƒ«ã«å¼·åŒ–å­¦ç¿’ã—ãŸã ã‘ã§ã¯é™ç•ŒãŒã‚ã£ãŸãŒã€è’¸ç•™ã‚’ã™ã‚‹ã¨é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ãŸ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23eb911",
   "metadata": {},
   "source": [
    "![](image/table6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866f5d6",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25414081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ğŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ğŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ğŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ğŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ğŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "NVIDIA_SMI = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout\n",
    "logging.info(NVIDIA_SMI)\n",
    "logging.info(f\"Python {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce29f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12345\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40811ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    if not os.path.exists(\"/content/open-r1\"):\n",
    "        !git clone https://github.com/huggingface/open-r1.git\n",
    "    %cd /content/open-r1\n",
    "    %pip install -e \".[dev]\" --no-deps\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    # !apt update && apt install git-lfs -y\n",
    "    if not os.path.exists(\"/workspaces/open-r1-distill/open-r1\"):\n",
    "        !git clone https://github.com/huggingface/open-r1.git\n",
    "    %cd /workspaces/open-r1-distill/open-r1\n",
    "    %pip install -e \".[dev]\" --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca732ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) PyTorchã¨Transformersã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "%pip install torch==2.6.0 transformers==4.52.3\n",
    "\n",
    "# 2) vLLMã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "%pip install vllm==0.8.5.post1\n",
    "\n",
    "# 3) Flash Attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# 2.8.3ã¯undefined symbolã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ãŸã‚2.7.3ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# https://github.com/Dao-AILab/flash-attention/issues/1832\n",
    "%pip install \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl\" --no-build-isolation\n",
    "\n",
    "# 4) ãã®ä»–ã®å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "%pip install \\\n",
    "    accelerate==1.4.0 \\\n",
    "    beautifulsoup4 \\\n",
    "    \"async-lru\" \\\n",
    "    bitsandbytes \\\n",
    "    \"distilabel[vllm]\" \\\n",
    "    deepspeed==0.16.8 \\\n",
    "    hf_transfer \\\n",
    "    langdetect \\\n",
    "    latex2sympy2_extended \\\n",
    "    liger-kernel \\\n",
    "    \"trl[vllm]==0.18.0\" \\\n",
    "    math-verify==0.5.2 \\\n",
    "    wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c803c6",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97498f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from distilabel.models import vLLM\n",
    "from distilabel.pipeline import Pipeline\n",
    "from distilabel.steps.tasks import TextGeneration\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaef900c",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c234e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 72400ä»¶ã®æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "# https://huggingface.co/datasets/AI-MO/NuminaMath-TIR\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”¨ã«å°‘ã—ã ã‘å–å¾—\n",
    "dataset = load_dataset(\n",
    "    \"AI-MO/NuminaMath-TIR\",\n",
    "    split=\"train\",\n",
    ").select(range(10))\n",
    "\n",
    "logger.info(f\"{len(dataset)=} {dataset[0].keys()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99118f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problemã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º\n",
    "logger.info(f\"problem:\\n{dataset[0]['problem']=}\")\n",
    "\n",
    "# (3/5x - 2/y)^8ã®å±•é–‹å¼ã«ãŠã‘ã‚‹x^2y^6ã®ä¿‚æ•°ã‚’åˆ†æ•°ã§æ±‚ã‚ã‚ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solutionã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º\n",
    "logger.info(f\"{dataset[0]['solution']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84007cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messagesã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º\n",
    "logger.info(f'{dataset[0][\"messages\"]=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd70d2b",
   "metadata": {},
   "source": [
    "### ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§æ¨è«–ã—ã€æœ€çµ‚å›ç­”ã‚’\\boxed{}ã§å›²ã£ã¦ãã ã•ã„\n",
    "prompt_template = \"\"\"\\\n",
    "You will be given a problem. Please reason step by step, and put your final answer within \\boxed{}:\n",
    "{{ instruction }}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è’¸ç•™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ\n",
    "\n",
    "# è’¸ç•™å…ƒã®ãƒ¢ãƒ‡ãƒ«\n",
    "# https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "with Pipeline(\n",
    "    name=\"distill-qwen-1.5b-r1\",\n",
    "    description=\"A pipeline to generate data from a distilled r1 model\",\n",
    ") as pipeline:\n",
    "\n",
    "    # distilabelã®vLLMã‚’åˆæœŸåŒ–\n",
    "    # https://distilabel.argilla.io/dev/components-gallery/llms/vllm/?h=vllm\n",
    "    llm = vLLM(\n",
    "        model=model_id,\n",
    "        tokenizer=model_id,\n",
    "        extra_kwargs={\n",
    "            \"tensor_parallel_size\": 1, # ãƒ¢ãƒ‡ãƒ«åˆ†å‰²ã®ã‚µã‚¤ã‚º\n",
    "            \"max_model_len\": 1024, # ãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼ˆ8192ï¼‰\n",
    "        },\n",
    "        generation_kwargs={\n",
    "            \"temperature\": 0.6, # ç”Ÿæˆæ™‚ã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            \"max_new_tokens\": 1024, # æ–°ã—ãç”Ÿæˆã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€å¤§æ•°ï¼ˆ8192ï¼‰\n",
    "        },\n",
    "    )\n",
    "\n",
    "    prompt_column = \"problem\"\n",
    "\n",
    "    text_generation = TextGeneration(\n",
    "        llm=llm, \n",
    "        template=prompt_template,\n",
    "        num_generations=4, # 1ã¤ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦4ã¤ç”Ÿæˆ\n",
    "        input_mappings={\"instruction\": prompt_column} if prompt_column is not None else {}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214396f9",
   "metadata": {},
   "source": [
    "### ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4296788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distiset = pipeline.run(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec761766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 * 4 = 40ä»¶ãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã‚‹\n",
    "logger.info(f\"{distiset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è’¸ç•™å…ƒã®ãƒ¢ãƒ‡ãƒ«å\n",
    "logger.info(f\"{distiset['default']['train'][0]['model_name']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425af5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "logger.info(f\"{distiset['default']['train'][0]['distilabel_metadata']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆçµæœ\n",
    "logger.info(f\"{distiset['default']['train'][0]['generation']=}\")\n",
    "\n",
    "# æ­£è§£ã‚’æ¤œè¨¼ã™ã‚‹å®Ÿè£…ãŒå¿…è¦ï¼ˆrejection samplingãªã©ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d4779",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98223586",
   "metadata": {},
   "source": [
    "```sh\n",
    "python src/open_r1/sft.py \\\n",
    "    --config recipes/OpenR1-Distill-7B/sft/config_distill.yaml \\\n",
    "    --model_name_or_path Qwen/Qwen3-0.6B-Base \\\n",
    "    --hub_model_id OpenR1-Distill-0.6B \\\n",
    "    --output_dir data/OpenR1-Distill-0.6B \\\n",
    "    --push_to_hub False \\\n",
    "    --report_to none\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from datasets import DatasetDict, concatenate_datasets\n",
    "from datasets import load_dataset\n",
    "from functools import partial, update_wrapper\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer\n",
    "from transformers import set_seed\n",
    "from transformers import TrainerCallback\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.training_args import TrainingArguments\n",
    "from trl import GRPOTrainer, ModelConfig, TrlParser, get_peft_config\n",
    "from trl import ModelConfig, get_kbit_device_map, get_quantization_config\n",
    "from trl import ModelConfig, SFTTrainer, TrlParser, get_peft_config, setup_chat_format\n",
    "from typing import Any, Callable, Dict, Literal, List, Optional\n",
    "import asyncio\n",
    "import datasets\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "\n",
    "logger.info(f\"transformers: {transformers.__version__}\") # 4.52.3\n",
    "logger.info(f\"torch: {torch.__version__}\") # 2.6.0+cu124\n",
    "logger.info(f\"trl: {trl.__version__}\") # 0.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4afab7e",
   "metadata": {},
   "source": [
    "### è¨­å®šã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9dbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments(trl.ScriptArguments):\n",
    "    \"\"\"\n",
    "    TRLã®ScriptArgumentsã‚’æ‹¡å¼µã—ãŸã‚¯ãƒ©ã‚¹\n",
    "    è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ··ãœã¦ã€å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ãŸã‚ã®æ©Ÿèƒ½ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    # Override the dataset_name to make it optional\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã€‚dataset_mixtureãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸è¦ã€‚\"}\n",
    "    )\n",
    "\n",
    "    dataset_mixture: Optional[dict[str, Any]] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ··ãœã¦ä½¿ç”¨ã™ã‚‹ãŸã‚ã®è¨­å®šã€‚ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã©ã®é«˜åº¦ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å«ã‚€ã“ã¨ãŒã§ãã‚‹ã€‚\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        åˆæœŸåŒ–å¾Œã®æ¤œè¨¼\n",
    "        \"\"\"\n",
    "\n",
    "        if self.dataset_name is None and self.dataset_mixture is None:\n",
    "            raise ValueError(\"Either `dataset_name` or `dataset_mixture` must be provided\")\n",
    "\n",
    "        if self.dataset_mixture is not None:\n",
    "            if not isinstance(self.dataset_mixture, dict) or \"datasets\" not in self.dataset_mixture:\n",
    "                raise ValueError(\n",
    "                    \"dataset_mixture must be a dictionary with a 'datasets' key. \"\n",
    "                    \"Expected format: {'datasets': [...], 'seed': int}\"\n",
    "                )\n",
    "\n",
    "            datasets_list = []\n",
    "            datasets_data = self.dataset_mixture.get(\"datasets\", [])\n",
    "\n",
    "            if isinstance(datasets_data, list):\n",
    "                for dataset_config in datasets_data:\n",
    "                    datasets_list.append(\n",
    "                        DatasetConfig(\n",
    "                            id=dataset_config.get(\"id\"),\n",
    "                            config=dataset_config.get(\"config\"),\n",
    "                            split=dataset_config.get(\"split\", \"train\"),\n",
    "                            columns=dataset_config.get(\"columns\"),\n",
    "                            weight=dataset_config.get(\"weight\", 1.0),\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError(\"'datasets' must be a list of dataset configurations\")\n",
    "\n",
    "            self.dataset_mixture = DatasetMixtureConfig(\n",
    "                datasets=datasets_list,\n",
    "                seed=self.dataset_mixture.get(\"seed\", 0),\n",
    "                test_split_size=self.dataset_mixture.get(\"test_split_size\", None),\n",
    "            )\n",
    "\n",
    "            # Check that column names are consistent across all dataset configs\n",
    "            columns_sets = [set(dataset.columns) for dataset in datasets_list if dataset.columns is not None]\n",
    "            if columns_sets:\n",
    "                first_columns = columns_sets[0]\n",
    "                if not all(columns == first_columns for columns in columns_sets):\n",
    "                    raise ValueError(\n",
    "                        \"Column names must be consistent across all dataset configurations in a mixture. \"\n",
    "                        f\"Found different column sets: {[list(cols) for cols in columns_sets]}\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SFTConfig(trl.SFTConfig):\n",
    "    \"\"\"\n",
    "    SFTã®è¨­å®šã‚¯ãƒ©ã‚¹\n",
    "    TRLã®SFTConfigã«ã‚«ã‚¹ã‚¿ãƒ ã®å¼•æ•°ã‚’è¿½åŠ ã—ã¦ã„ã‚‹\n",
    "\n",
    "    - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š\n",
    "    - è¨“ç·´ä¸­ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š\n",
    "    - ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­å®š\n",
    "    - ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š\n",
    "    - Hubã®ãƒªãƒ“ã‚¸ãƒ§ãƒ³è¨­å®š\n",
    "    - Weights & Biasesè¨­å®š\n",
    "    \"\"\"\n",
    "\n",
    "    benchmarks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã«å®Ÿè¡Œã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ãƒªã‚¹ãƒˆ\"},\n",
    "    )\n",
    "\n",
    "    callbacks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«å®Ÿè¡Œã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ãƒªã‚¹ãƒˆ\"},\n",
    "    )\n",
    "\n",
    "    chat_template: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"ä½¿ç”¨ã™ã‚‹ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\"}\n",
    "    )\n",
    "\n",
    "    system_prompt: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ä½¿ç”¨ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\"},\n",
    "    )\n",
    "\n",
    "    hub_model_revision: Optional[str] = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹Hubã®ãƒ–ãƒ©ãƒ³ãƒ\"},\n",
    "    )\n",
    "\n",
    "    overwrite_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Hubã®ãƒªãƒ“ã‚¸ãƒ§ãƒ³ã‚’ä¸Šæ›¸ãã™ã‚‹ã‹ã©ã†ã‹\"}\n",
    "    )\n",
    "\n",
    "    push_to_hub_revision: bool = field(\n",
    "        default=False, metadata={\"help\": \"Hubã®ãƒªãƒ“ã‚¸ãƒ§ãƒ³/ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã‹ã©ã†ã‹\"}\n",
    "    )\n",
    "\n",
    "    wandb_entity: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"å®Ÿè¡Œã‚’ä¿å­˜ã™ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£\")},\n",
    "    )\n",
    "\n",
    "    wandb_project: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"å®Ÿè¡Œã‚’ä¿å­˜ã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ\")},\n",
    "    )\n",
    "\n",
    "    wandb_run_group: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"å®Ÿè¡Œã‚’ä¿å­˜ã™ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—\")},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89222744",
   "metadata": {},
   "source": [
    "### ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(args: ScriptArguments) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚ã—ãã¯æ··åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆmixture of datasetsï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—åˆæœŸåŒ–ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        args (ScriptArguments): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨­å®šã‚’å«ã‚€ã‚¹ã‚¯ãƒªãƒ—ãƒˆå¼•æ•°\n",
    "    Returns:\n",
    "        DatasetDict: èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
    "    if args.dataset_name and not args.dataset_mixture:\n",
    "        logger.debug(f\"å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ {args.dataset_name} \")\n",
    "        return datasets.load_dataset(args.dataset_name, args.dataset_config)\n",
    "\n",
    "    # æ··åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\n",
    "    elif args.dataset_mixture:\n",
    "        logger.debug(f\"æ··åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ {len(args.dataset_mixture.datasets)}\")\n",
    "        seed = args.dataset_mixture.seed\n",
    "        datasets_list = []\n",
    "\n",
    "        # æ§‹æˆè¦ç´ ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ«ãƒ¼ãƒ—\n",
    "        for dataset_config in args.dataset_mixture.datasets:\n",
    "            logger.debug(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ {dataset_config.id} (config: {dataset_config.config})\")\n",
    "\n",
    "            # å˜ä¸€ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿\n",
    "            ds = datasets.load_dataset(\n",
    "                dataset_config.id,\n",
    "                dataset_config.config,\n",
    "                split=dataset_config.split,\n",
    "            )\n",
    "\n",
    "            # åˆ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "            if dataset_config.columns is not None:\n",
    "\n",
    "                # åˆ—ã‚’é¸æŠ\n",
    "                ds = ds.select_columns(dataset_config.columns)\n",
    "\n",
    "            # é‡ã¿ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "            if dataset_config.weight is not None:\n",
    "\n",
    "                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã€é‡ã¿ã«åŸºã¥ã„ã¦ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "                ds = ds.shuffle(seed=seed).select(\n",
    "                    range(int(len(ds) * dataset_config.weight))\n",
    "                )\n",
    "\n",
    "                logger.debug(\n",
    "                    f\"é‡ã¿ä»˜ãã§ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{dataset_config.id}' (config: {dataset_config.config}) with weight={dataset_config.weight} to {len(ds)} examples\"\n",
    "                )\n",
    "\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "            datasets_list.append(ds)\n",
    "\n",
    "        # æ­£å¸¸ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒèª­ã¿è¾¼ã¾ã‚ŒãŸå ´åˆ\n",
    "        if datasets_list:\n",
    "\n",
    "            # ãƒªã‚¹ãƒˆã‚’é€£çµã—ã¦æ··åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
    "            combined_dataset = concatenate_datasets(datasets_list)\n",
    "\n",
    "            # æ··åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "            combined_dataset = combined_dataset.shuffle(seed=seed)\n",
    "            logger.debug(f\"æ··åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ {len(combined_dataset)} examples\")\n",
    "\n",
    "            # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«åˆ†å‰²ã™ã‚‹å ´åˆ\n",
    "            if args.dataset_mixture.test_split_size is not None:\n",
    "\n",
    "                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆã«åˆ†å‰²\n",
    "                combined_dataset = combined_dataset.train_test_split(\n",
    "                    test_size=args.dataset_mixture.test_split_size, seed=seed\n",
    "                )\n",
    "\n",
    "                logger.debug(\n",
    "                    f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ã‚»ãƒƒãƒˆ {len(combined_dataset['train'])} examples ã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ {len(combined_dataset['test'])} examples ã«åˆ†å‰²\"\n",
    "                )\n",
    "\n",
    "                return combined_dataset\n",
    "\n",
    "            # åˆ†å‰²ã—ãªã„å ´åˆ\n",
    "            else:\n",
    "                return DatasetDict({\"train\": combined_dataset})\n",
    "        else:\n",
    "            raise ValueError(\"No datasets were loaded from the mixture configuration\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Either `dataset_name` or `dataset_mixture` must be provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_args: ModelConfig, training_args: Any) -> PreTrainedTokenizer:\n",
    "    \"\"\"\n",
    "    äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€åˆæœŸåŒ–ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        model_args (ModelConfig): ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "        training_args (Any): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\n",
    "    Returns:\n",
    "        PreTrainedTokenizer: åˆæœŸåŒ–ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "    \"\"\"\n",
    "    logger.info(f\"Tokenizerã‚’åˆæœŸåŒ– {model_args.model_name_or_path=}\")\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "    )\n",
    "    logger.debug(f\"Tokenizerã®è¨­å®š: {tokenizer}\")\n",
    "\n",
    "    # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "    if training_args.chat_template is not None:\n",
    "        logger.debug(f\"ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è¨­å®š: {training_args.chat_template}\")\n",
    "        tokenizer.chat_template = training_args.chat_template\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03433ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_args: ModelConfig, training_args: SFTConfig) -> AutoModelForCausalLM:\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        model_args (ModelConfig): ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š\n",
    "        training_args (SFTConfig, GRPOConfig): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®è¨­å®š\n",
    "    Returns:\n",
    "        AutoModelForCausalLM: åˆæœŸåŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«\n",
    "    \"\"\"\n",
    "    logger.info(f\"ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ– {model_args.model_name_or_path}\")\n",
    "\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype \\\n",
    "            if model_args.torch_dtype in [\"auto\", None] \\\n",
    "            else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    logger.debug(f\"{torch_dtype=}\")\n",
    "\n",
    "    quantization_config = get_quantization_config(model_args)\n",
    "    logger.debug(f\"{quantization_config=}\")\n",
    "\n",
    "    model_kwargs = dict(\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        attn_implementation=model_args.attn_implementation,\n",
    "        torch_dtype=torch_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,\n",
    "        device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    logger.debug(f\"{model_kwargs=}\")\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        **model_kwargs,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23024e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(train_config, model_config) -> List[TrainerCallback]:\n",
    "    \"\"\"\n",
    "    å­¦ç¿’ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒä¿å­˜ã•ã‚Œã‚‹ãŸã³ã«å®Ÿè¡Œã•ã‚Œã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã‚’å–å¾—ã™ã‚‹\n",
    "    æ–°ã—ã„ãƒªãƒ“ã‚¸ãƒ§ãƒ³ã§ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ãªã©\n",
    "    ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "    \"\"\"\n",
    "\n",
    "    callbacks = []\n",
    "\n",
    "    # for callback_name in train_config.callbacks:\n",
    "    #     if callback_name not in CALLBACKS:\n",
    "    #         raise ValueError(f\"Callback {callback_name} not found in CALLBACKS.\")\n",
    "    #     callbacks.append(CALLBACKS[callback_name](model_config))\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wandb_training(training_args):\n",
    "    \"\"\"\n",
    "    Helper function for setting up Weights & Biases logging tools.\n",
    "    \"\"\"\n",
    "    if training_args.wandb_entity is not None:\n",
    "        os.environ[\"WANDB_ENTITY\"] = training_args.wandb_entity\n",
    "    if training_args.wandb_project is not None:\n",
    "        os.environ[\"WANDB_PROJECT\"] = training_args.wandb_project\n",
    "    if training_args.wandb_run_group is not None:\n",
    "        os.environ[\"WANDB_RUN_GROUP\"] = training_args.wandb_run_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb7f09",
   "metadata": {},
   "source": [
    "### è¨“ç·´è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š\n",
    "\n",
    "model_args = ModelConfig(\n",
    "    # æœ¬æ¥ã¯\"open-r1/Qwen2.5-Math-7B-RoPE-300k\"\n",
    "    # RoPEã®å‘¨æ³¢æ•°ã‚’300kã«æ‹¡å¼µã—ãŸQwen2.5-Math-7Bãƒ¢ãƒ‡ãƒ«\n",
    "    # SFTä¸­ã«ã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆé•·ã‚’4kã‹ã‚‰32kã«æ‹¡å¼µã™ã‚‹ãŸã‚\n",
    "    # https://huggingface.co/open-r1/Qwen2.5-Math-7B-RoPE-300k\n",
    "    model_name_or_path=\"Qwen/Qwen3-0.6B-Base\",\n",
    "    model_revision=\"main\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTã®è¨“ç·´è¨­å®š\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    bf16=True,\n",
    "    do_eval=False,\n",
    "    eval_strategy=\"no\",\n",
    "    gradient_accumulation_steps=32, # æœ¬æ¥ã¯\"8\"ã ãŒã€ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’å›é¿ã™ã‚‹ãŸã‚ã«å¢—åŠ \n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    hub_model_id=\"OpenR1-Distill-0.6B\", # \"OpenR1-Distill-7B\"\n",
    "    hub_strategy=\"every_save\",\n",
    "    learning_rate=4.0e-5,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
    "    packing=False,\n",
    "    max_grad_norm=0.2,\n",
    "    max_length=32768, # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’32kã«æ‹¡å¼µ\n",
    "    max_steps=1, # æœ¬æ¥ã¯\"-1\"ã ãŒã€ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ç”¨ã«å°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«è¨­å®š\n",
    "    num_train_epochs=5, # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’5é †ã™ã‚‹\n",
    "    output_dir=\"data/OpenR1-Distill-0.6B\", # \"data/OpenR1-Distill-7B\"\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    push_to_hub=False, # æœ¬æ¥ã¯Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚True\n",
    "    report_to=[], # æœ¬æ¥ã¯ãƒ­ã‚°ã‚’ç›£è¦–ã™ã‚‹ãŸã‚[\"wandb\"]\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    use_liger_kernel=True,\n",
    "    warmup_ratio=0.03,\n",
    "    dataset_num_proc=12,\n",
    "    eos_token=\"<|im_end|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1361e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨­å®š\n",
    "\n",
    "script_args = ScriptArguments(\n",
    "    # 349,000ä»¶ã®CoTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆcode + math + science)\n",
    "    # https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts\n",
    "    dataset_name=\"open-r1/Mixture-of-Thoughts\",\n",
    "    dataset_config=\"math\", # æœ¬æ¥ã¯\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformersã¨torchã®ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¸ˆã¿ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã€å†é–‹\n",
    "\n",
    "last_checkpoint = None\n",
    "\n",
    "if os.path.isdir(training_args.output_dir):\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\n",
    "if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "    logger.info(f\"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œå‡ºã€{last_checkpoint=} ã‹ã‚‰å­¦ç¿’ã‚’å†é–‹ã—ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & BiasesãŒæœ‰åŠ¹ãªå ´åˆã€åˆæœŸåŒ–\n",
    "\n",
    "if \"wandb\" in training_args.report_to:\n",
    "    init_wandb_training(training_args)\n",
    "    logger.info(\"Weights & Biasesã®åˆæœŸåŒ–ãŒå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65a680",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88323001",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(script_args)\n",
    "logger.info(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾— {dataset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ç”¨ã«ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].select(range(1000))\n",
    "\n",
    "logger.info(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° {dataset=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028ecc6",
   "metadata": {},
   "source": [
    "### ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb95957",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(model_args, training_args)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è¨­å®š\n",
    "\n",
    "if tokenizer.chat_template is None:\n",
    "    logger.info(\"No chat template provided, defaulting to ChatML.\")\n",
    "    model, tokenizer = setup_chat_format(model, tokenizer, format=\"chatml\")\n",
    "\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d578534",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(model_args, training_args)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286604dd",
   "metadata": {},
   "source": [
    "### SFTTrainerã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[script_args.dataset_train_split]\n",
    "logger.info(f\"{train_dataset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910bb435",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = (dataset[script_args.dataset_test_split] \\\n",
    "    if training_args.eval_strategy != \"no\" else None)\n",
    "logger.info(f\"{eval_dataset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = get_peft_config(model_args)\n",
    "logger.info(f\"{peft_config=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa34b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(training_args, model_args)\n",
    "logger.info(f\"{callbacks=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTTrainerã®åˆæœŸåŒ–\n",
    "# - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‰å‡¦ç†ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "# - æ­£è§£ãƒ©ãƒ™ãƒ«ã®ä½œæˆã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã®ä½œæˆ\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d7f8f",
   "metadata": {},
   "source": [
    "### è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48698aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "elif last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "\n",
    "logger.info(f\"è¨“ç·´ã®å†é–‹ã«ä½¿ç”¨ã™ã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ {checkpoint=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75af911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ã®å®Ÿè¡Œ\n",
    "# 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒãƒƒãƒã‚’å–å¾—\n",
    "# 2. ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­ã‚’å®Ÿè¡Œã—ã€æå¤±ã‚’è¨ˆç®—ã™ã‚‹\n",
    "# 3. é€†ä¼æ’­ã—ã€å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹\n",
    "# 4. æœ€é©åŒ–é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹\n",
    "# å®šæœŸçš„ã«è©•ä¾¡ã—ãŸã‚Šã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ãŸã‚Šã™ã‚‹\n",
    "\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43e34a",
   "metadata": {},
   "source": [
    "### ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡º\n",
    "metrics = train_result.metrics\n",
    "\n",
    "# ã€Œå­¦ç¿’ã«ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ç·æ•°ã€ã‚’çµ±è¨ˆã«è¿½åŠ \n",
    "metrics[\"train_samples\"] = len(dataset[script_args.dataset_train_split])\n",
    "\n",
    "# ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "\n",
    "# JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
    "trainer.save_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8080fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainerã®å†…éƒ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜\n",
    "# - æœ€é©åŒ–é–¢æ•°ã®çŠ¶æ…‹\n",
    "# - ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®çŠ¶æ…‹\n",
    "# - ä¹±æ•°ã‚·ãƒ¼ãƒ‰ãªã©\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267fdc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆè¨­å®šã®EOSãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®EOSãƒˆãƒ¼ã‚¯ãƒ³IDã«è¨­å®š\n",
    "# ç„¡é™ç”Ÿæˆã‚’é˜²æ­¢ã™ã‚‹ãŸã‚\n",
    "trainer.model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "trainer.save_model(training_args.output_dir)\n",
    "logger.info(f\"å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a5610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãã®ä»–ã®æƒ…å ±ã‚’ä¿å­˜\n",
    "\n",
    "if trainer.accelerator.is_main_process:\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚’ä½œæˆ\n",
    "    kwargs = {\n",
    "        \"dataset_name\": script_args.dataset_name,\n",
    "        \"tags\": [\"open-r1\"],\n",
    "    }\n",
    "    trainer.create_model_card(**kwargs)\n",
    "\n",
    "    # æ¨è«–ã®ãŸã‚ã«KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ–ã—ã€é«˜é€ŸåŒ–\n",
    "    trainer.model.config.use_cache = True\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’ä¿å­˜ï¼ˆconfig.jsonï¼‰\n",
    "    trainer.model.config.save_pretrained(training_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©•ä¾¡ã®å®Ÿè¡Œ\n",
    "\n",
    "if training_args.do_eval:\n",
    "    metrics = trainer.evaluate()\n",
    "    metrics[\"eval_samples\"] = len(dataset[script_args.dataset_test_split])\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hubã«ãƒ—ãƒƒã‚·ãƒ¥\n",
    "\n",
    "if training_args.push_to_hub:\n",
    "    trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560407d0",
   "metadata": {},
   "source": [
    "### ãƒ¡ãƒ¢ãƒªé–‹æ”¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b819cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in [\"dataset\", \"tokenizer\", \"model\", \"trainer\", \"trainer_result\", \"mertics\"]:\n",
    "    try:\n",
    "        del globals()[target]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996b621",
   "metadata": {},
   "source": [
    "## GRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce55fb",
   "metadata": {},
   "source": [
    "```sh\n",
    "!RANK=0 WORLD_SIZE=1 LOCAL_RANK=0 MASTER_ADDR=localhost MASTER_PORT=12345 python src/open_r1/grpo.py \\\n",
    "    --config recipes/DeepSeek-R1-Distill-Qwen-1.5B/grpo/config_demo.yaml \\\n",
    "    --model_name_or_path Qwen/Qwen3-0.6B-Base \\\n",
    "    --hub_model_id OpenR1-Distill-0.6B \\\n",
    "    --output_dir data/OpenR1-Distill-0.6B \\\n",
    "    --push_to_hub False \\\n",
    "    --report_to none \\\n",
    "    --vllm_mode colocate \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --num_generations 4 \\\n",
    "    --gradient_accumulation_steps 8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from datasets import DatasetDict, concatenate_datasets\n",
    "from datasets import load_dataset\n",
    "from functools import partial, update_wrapper\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer\n",
    "from transformers import set_seed\n",
    "from transformers import TrainerCallback\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.training_args import TrainingArguments\n",
    "from trl import GRPOTrainer, ModelConfig, TrlParser, get_peft_config\n",
    "from trl import ModelConfig, get_kbit_device_map, get_quantization_config\n",
    "from trl import ModelConfig, SFTTrainer, TrlParser, get_peft_config, setup_chat_format\n",
    "from typing import Any, Callable, Dict, Literal, List, Optional\n",
    "import asyncio\n",
    "import datasets\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import transformers\n",
    "import trl\n",
    "import torch\n",
    "\n",
    "logger.info(f\"transformers: {transformers.__version__}\") # 4.52.3\n",
    "logger.info(f\"torch: {torch.__version__}\") # 2.6.0+cu124\n",
    "logger.info(f\"trl: {trl.__version__}\") # 0.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd28d2e",
   "metadata": {},
   "source": [
    "### è¨­å®šã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a19b3ce",
   "metadata": {},
   "source": [
    "#### GRPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d502a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOConfig(trl.GRPOConfig):\n",
    "    \"\"\"\n",
    "    GRPOã®è¨­å®šã‚¯ãƒ©ã‚¹\n",
    "    TRLã®GRPOConfigã«ã‚«ã‚¹ã‚¿ãƒ ã®å¼•æ•°ã‚’è¿½åŠ ã—ã¦ã„ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    benchmarks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"è¨“ç·´å¾Œã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆ\"},\n",
    "    )\n",
    "\n",
    "    callbacks: list[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"help\": \"è¨“ç·´ä¸­ã«å®Ÿè¡Œã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ãƒªã‚¹ãƒˆ\"},\n",
    "    )\n",
    "\n",
    "    chat_template: Optional[str] = field(default=None, metadata={\n",
    "        \"help\": \"ä½¿ç”¨ã™ã‚‹ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\"\n",
    "    })\n",
    "\n",
    "    hub_model_revision: Optional[str] = field(\n",
    "        default=\"main\", metadata={\"help\": \"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹Hubã®ãƒ–ãƒ©ãƒ³ãƒ\"}\n",
    "    )\n",
    "\n",
    "    num_completions_to_print: int = field(default=0, metadata={\n",
    "        \"help\": \"è¡¨ç¤ºã™ã‚‹å®Œäº†æ•°\"\n",
    "    })\n",
    "\n",
    "    overwrite_hub_revision: bool = field(default=False, metadata={\n",
    "        \"help\": \"Hubã®ãƒªãƒ“ã‚¸ãƒ§ãƒ³ã‚’ä¸Šæ›¸ãã™ã‚‹ã‹ã©ã†ã‹\"\n",
    "    })\n",
    "\n",
    "    push_to_hub_revision: bool = field(default=False, metadata={\n",
    "        \"help\": \"Hubã®ãƒªãƒ“ã‚¸ãƒ§ãƒ³/ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã‹ã©ã†ã‹\"\n",
    "    })\n",
    "\n",
    "    system_prompt: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"ä½¿ç”¨ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\"},\n",
    "    )\n",
    "\n",
    "    wandb_log_unique_prompts: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’wandbã«ãƒ­ã‚°ã™ã‚‹ã‹ã©ã†ã‹ã€‚å„ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã”ã¨ã«æ–°ã—ã„ãƒ©ãƒ³ãŒä½œæˆã•ã‚Œã¾ã™ã€‚\"\n",
    "        },\n",
    "    )\n",
    "    wandb_entity: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"ãƒ©ãƒ³ã‚’ä¿å­˜ã™ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£\"},\n",
    "    )\n",
    "\n",
    "    wandb_project: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"ãƒ©ãƒ³ã‚’ä¿å­˜ã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ\"},\n",
    "    )\n",
    "\n",
    "    wandb_run_group: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"ãƒ©ãƒ³ã‚’ä¿å­˜ã™ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e555ae4",
   "metadata": {},
   "source": [
    "#### GRPOScriptArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09213c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOScriptArguments(ScriptArguments):\n",
    "    \"\"\"\n",
    "    GRPOã®è¨“ç·´è¨­å®š\n",
    "    \"\"\"\n",
    "\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"accuracy\", \"format\", \"tag_count\"],\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"å ±é…¬é–¢æ•°ã®ãƒªã‚¹ãƒˆã€‚å¯èƒ½ãªå€¤: 'accuracy'ï¼ˆæ­£ç¢ºæ€§ï¼‰ã€'format'ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰ã€'reasoning_steps'ï¼ˆæ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼‰ã€'cosine'ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰ã€'repetition_penalty'ï¼ˆç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰ã€'length'ï¼ˆé•·ã•ï¼‰ã€'tag_count'ï¼ˆã‚¿ã‚°æ•°ï¼‰ã€'code'ï¼ˆã‚³ãƒ¼ãƒ‰ï¼‰ã€'code_format'ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    cosine_min_value_wrong: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"é–“é•ãˆãŸå›ç­”ã«å¯¾ã™ã‚‹æœ€å°å ±é…¬\"},\n",
    "    )\n",
    "\n",
    "    cosine_max_value_wrong: float = field(\n",
    "        default=-0.5,\n",
    "        metadata={\"help\": \"é–“é•ãˆãŸå›ç­”ã«å¯¾ã™ã‚‹æœ€å¤§å ±é…¬\"},\n",
    "    )\n",
    "\n",
    "    cosine_min_value_correct: float = field(\n",
    "        default=0.5,\n",
    "        metadata={\"help\": \"æ­£ã—ã„å›ç­”ã«å¯¾ã™ã‚‹æœ€å°å ±é…¬\"},\n",
    "    )\n",
    "\n",
    "    cosine_max_value_correct: float = field(\n",
    "        default=1.0,\n",
    "        metadata={\"help\": \"æ­£ã—ã„å›ç­”ã«å¯¾ã™ã‚‹æœ€å¤§å ±é…¬\"},\n",
    "    )\n",
    "\n",
    "    cosine_max_len: int = field(\n",
    "        default=1000,\n",
    "        metadata={\"help\": \"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æœ€å¤§é•·\"},\n",
    "    )\n",
    "\n",
    "    repetition_n_grams: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"n-gramã®é•·ã•ã‚’ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£å ±é…¬ã«ä½¿ç”¨\"},\n",
    "    )\n",
    "\n",
    "    repetition_max_penalty: float = field(\n",
    "        default=-1.0,\n",
    "        metadata={\"help\": \"ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£å ±é…¬ã®æœ€å¤§å€¤\"},\n",
    "    )\n",
    "\n",
    "    code_language: str = field(\n",
    "        default=\"python\",\n",
    "        # '(?:python|cpp)'\n",
    "        metadata={\n",
    "            \"help\": \"ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå ±é…¬ã®ãŸã‚ã®è¨€èªã€‚E2BãŒã‚µãƒãƒ¼ãƒˆã™ã‚‹è¨€èªã«åŸºã¥ã https://e2b.dev/docs/code-interpreting/supported-languages\",\n",
    "            \"choices\": [\"python\", \"javascript\", \"r\", \"java\", \"bash\", \"cpp\"],\n",
    "        },\n",
    "    )\n",
    "    code_eval_test_batch_size: int = field(\n",
    "        default=1,\n",
    "        metadata={\n",
    "            \"help\": \"å„ç”Ÿæˆã«å¯¾ã—ã¦ã€ã“ã‚Œã ã‘ã®æ•°ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ä¸¦åˆ—ã§è©•ä¾¡ã—ã€ãã®ã†ã¡ã®ã„ãšã‚Œã‹ãŒå¤±æ•—ï¼ˆã‚¹ã‚³ã‚¢0ï¼‰ã—ãŸå ´åˆã¯è©•ä¾¡ã‚’åœæ­¢ã—ã€ãã†ã§ãªã‘ã‚Œã°æ¬¡ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ãƒãƒƒãƒã«é€²ã‚€ã€‚è©•ä¾¡ã‚µãƒ¼ãƒãƒ¼ã®éè² è·ã‚’é¿ã‘ã€èª¤ã£ãŸè§£ç­”ã«å¯¾ã™ã‚‹æ™‚é–“ã‚’ç¯€ç´„ã™ã‚‹ã®ã«æœ‰ç”¨ã€‚\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    code_eval_scoring_mode: Literal[\"pass_fail\", \"partial\", \"weighted_sum\"] = field(\n",
    "        default=\"weighted_sum\",\n",
    "        metadata={\"help\": \"ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®åˆæ ¼å‰²åˆã‚’å ±é…¬ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚falseã®å ´åˆã¯0/1ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\"},\n",
    "    )\n",
    "\n",
    "    parallel_code_exec_per_proc: int = field(\n",
    "        default=2,\n",
    "        metadata={\n",
    "            \"help\": \"ãƒ—ãƒ­ã‚»ã‚¹ã”ã¨ã®ä¸¦åˆ—E2Bã‚³ãƒ¼ãƒ‰å®Ÿè¡Œæ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®2ã¯ã€8 GPUã‚’ä½¿ç”¨ã—ãŸE2Bã®Free Hobbyãƒ†ã‚£ã‚¢ã«é©ã—ã¦ã„ã¾ã™ã€‚\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    dataset_prompt_column: str = field(\n",
    "        default=\"prompt\",\n",
    "        metadata={\"help\": \"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åˆ—åã€‚\"},\n",
    "    )\n",
    "\n",
    "    e2b_router_url: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"E2Bãƒ«ãƒ¼ã‚¿ãƒ¼ã®URLã€‚scripts/e2b_router.pyã‚’å‚ç…§ã—ã¦ãã ã•ã„\"},\n",
    "    )\n",
    "\n",
    "    morph_router_url: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"MorphCloudãƒ«ãƒ¼ã‚¿ãƒ¼ã®URLã€‚scripts/morph_router.pyã‚’å‚ç…§ã—ã¦ãã ã•ã„\"},\n",
    "    )\n",
    "\n",
    "    code_provider: Optional[str] = field(\n",
    "        default=\"e2b\",\n",
    "        metadata={\n",
    "            \"help\": \"ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã€‚ã‚ªãƒ—ã‚·ãƒ§ãƒ³: 'e2b', 'local', 'morph'\",\n",
    "            \"choices\": [\"e2b\", \"local\", \"morph\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ioi_provider: Optional[str] = field(\n",
    "        default=\"piston\",\n",
    "        metadata={\n",
    "            \"help\": \"IOIã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã€‚ã‚ªãƒ—ã‚·ãƒ§ãƒ³: 'piston', 'morph'\",\n",
    "            \"choices\": [\"piston\", \"morph\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    max_completion_len: int = field(\n",
    "        default=16384,\n",
    "        metadata={\"help\": \"è£œå®Œã®æœ€å¤§æ–‡å­—æ•°\"},\n",
    "    )\n",
    "\n",
    "    soft_punish_cache: int = field(\n",
    "        default=4096,\n",
    "        metadata={\"help\": \"è£œå®Œã®æœ€å°æ–‡å­—æ•°\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657baa1",
   "metadata": {},
   "source": [
    "### å ±é…¬é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_reward(completions: list[list[dict[str, str]]], solution: list[str], **kwargs) -> list[Optional[float]]:\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã—ãŸå›ç­”ãŒã€æ­£è§£ã¨æ•°å­¦çš„ã«ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã‚’åˆ¤å®šã—ã€å ±é…¬ã‚’ä¸ãˆã‚‹é–¢æ•°\n",
    "\n",
    "    Args:\n",
    "        completions (list[list[dict[str, str]]]): ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã—ãŸå›ç­”ã®ãƒªã‚¹ãƒˆ\n",
    "        solution (list[str]): æ­£è§£ã®ãƒªã‚¹ãƒˆ\n",
    "    Returns:\n",
    "        list[Optional[float]]: å„å›ç­”ã«å¯¾ã™ã‚‹å ±é…¬ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) ç”Ÿæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆ(completion)ã¨æ­£è§£ãƒ‡ãƒ¼ã‚¿(solution)ã‚’å—ã‘å–ã‚‹\n",
    "\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    # 2) å„å›ç­”ã«ã¤ã„ã¦ã€æ­£è§£ã¨æ•°å­¦çš„ã«ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼ã—ã€å ±é…¬ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "    for content, sol in zip(contents, solution):\n",
    "\n",
    "        # æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ­£è§£ã‚’æŠ½å‡º\n",
    "        gold_parsed = parse(\n",
    "            sol,\n",
    "            extraction_mode=\"first_match\",\n",
    "        )\n",
    "\n",
    "        # è§£æå¯èƒ½ãªæ­£è§£ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        if len(gold_parsed) != 0:\n",
    "\n",
    "            # å›ç­”ã¯æ­£ã—ã„LaTeXå½¢å¼ã§æä¾›ã•ã‚Œã‚‹å¿…è¦ãŒã‚ã‚‹ï¼ˆä¸æ­£ãªæ¼”ç®—å­ã¯ä¸å¯ï¼‰\n",
    "            # math_verifyã®parseé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã€å›ç­”ã‹ã‚‰æ•°å­¦çš„è¡¨ç¾ã‚’æŠ½å‡º\n",
    "            answer_parsed = parse(\n",
    "                content,\n",
    "                extraction_config=[\n",
    "                    LatexExtractionConfig(\n",
    "                        normalization_config=NormalizationConfig(\n",
    "                            nits=False,\n",
    "                            malformed_operators=False,\n",
    "                            basic_latex=True,\n",
    "                            equations=True,\n",
    "                            boxed=\"all\", # \\boxed{}å†…ã®å¼ã‚’å„ªå…ˆçš„ã«æŠ½å‡º\n",
    "                            units=True,\n",
    "                        ),\n",
    "                        # Ensures that boxed is tried first\n",
    "                        boxed_match_priority=0,\n",
    "                        try_extract_without_anchor=False,\n",
    "                    )\n",
    "                ],\n",
    "                extraction_mode=\"first_match\",\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # math_verifyã®verifyé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã€å›ç­”ã¨æ­£è§£ãŒæ•°å­¦çš„ã«ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼\n",
    "                reward = float(verify(gold_parsed, answer_parsed))\n",
    "            except Exception as e:\n",
    "                # å¤±æ•—ã—ãŸå ´åˆã¯å ±é…¬ã‚’Noneã«è¨­å®šã—ã€ã‚¹ã‚­ãƒƒãƒ—\n",
    "                print(f\"verify failed: {e}, answer: {answer_parsed}, gold: {gold_parsed}\")\n",
    "                reward = None\n",
    "\n",
    "        # è§£æå¯èƒ½ãªæ­£è§£ãŒå­˜åœ¨ã—ãªã„å ´åˆ\n",
    "        else:\n",
    "            # ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãŸã‚æ˜ç¤ºçš„ã«Noneã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "            reward = None\n",
    "            print(\"Failed to parse gold solution: \", sol)\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "accuracy_reward(\n",
    "    completions=[\n",
    "        [{\"role\": \"assistant\", \"content\": \"Answer is \\\\boxed{42}\"}],\n",
    "    ], \n",
    "    solution=[\"42\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91711692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    æ€è€ƒéç¨‹ãŒ<think>ã¨</think>ã‚¿ã‚°ã§å›²ã¾ã‚Œã€æœ€çµ‚å›ç­”ãŒ<answer>ã¨</answer>ã‚¿ã‚°ã§\n",
    "    å›²ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹å ±é…¬é–¢æ•°\n",
    "\n",
    "    Args:\n",
    "        completions (list[list[dict[str, str]]]): ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã—ãŸå›ç­”ã®ãƒªã‚¹ãƒˆ\n",
    "    Returns:\n",
    "        list[float]: å„å›ç­”ã«å¯¾ã™ã‚‹å ±é…¬ã®ãƒªã‚¹ãƒˆ\n",
    "        ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒæ­£ã—ã„å ´åˆã¯1.0ã€ãã†ã§ãªã„å ´åˆã¯0.0ï¼‰\n",
    "    # \"\"\"\n",
    "\n",
    "    # æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®šç¾©\n",
    "    pattern = r\"^<think>\\n.*?\\n</think>\\n<answer>\\n.*?\\n</answer>$\"\n",
    "\n",
    "    # contentã‚­ãƒ¼ã®å€¤ã‚’æŠ½å‡º\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    # å„å›ç­”ãŒãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ã‹ã‚’ç¢ºèª\n",
    "    matches = [\n",
    "        re.match(pattern, content, re.DOTALL | re.MULTILINE)\n",
    "        for content in completion_contents\n",
    "    ]\n",
    "\n",
    "    # ãƒãƒƒãƒçµæœã«åŸºã¥ã„ã¦å ±é…¬ã‚’è¨ˆç®—\n",
    "    return [1.0 if match else 0.0 for match in matches]\n",
    "\n",
    "format_reward(\n",
    "    completions=[[\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"<think>\\nHold on...\\n</think>\\n<answer>\\n42\\n</answer>\"\n",
    "        }\n",
    "    ]], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febbd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_count_reward(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    format_reward()ã«é–¢é€£ã™ã‚‹<think>ãŠã‚ˆã³<answer>ã‚¿ã‚°ã®æ‰€æœ›ã®æ•°ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹ã®å ±é…¬\n",
    "    \"\"\"\n",
    "\n",
    "    # æ¡ç‚¹ã™ã‚‹é–¢æ•°\n",
    "    def count_tags(text: str) -> float:\n",
    "        count = 0.0\n",
    "\n",
    "        if text.count(\"<think>\\n\") == 1:\n",
    "            count += 0.25\n",
    "\n",
    "        if text.count(\"\\n</think>\\n\") == 1:\n",
    "            count += 0.25\n",
    "\n",
    "        if text.count(\"\\n<answer>\\n\") == 1:\n",
    "            count += 0.25\n",
    "\n",
    "        if text.count(\"\\n</answer>\") == 1:\n",
    "            count += 0.25\n",
    "\n",
    "        return count\n",
    "\n",
    "    # contentã‚­ãƒ¼ã®å€¤ã‚’æŠ½å‡º\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    # å„å›ç­”ã«å¯¾ã—ã¦ã‚¿ã‚°ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    return [count_tags(c) for c in contents]\n",
    "\n",
    "tag_count_reward(\n",
    "    completions=[[\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"<think>\\nHold on...\\n</think>\\n<answer>\\n42\\n\"\n",
    "        }\n",
    "    ]], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3949fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_funcs(script_args) -> list[Callable]:\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã•ã‚ŒãŸå ±é…¬é–¢æ•°ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        script_args: ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å¼•æ•°\n",
    "    Returns:\n",
    "        list[Callable]: å ±é…¬é–¢æ•°ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    REWARD_FUNCS_REGISTRY = {\n",
    "        \"accuracy\": accuracy_reward,\n",
    "        \"format\": format_reward,\n",
    "        # \"reasoning_steps\": reasoning_steps_reward,\n",
    "        # \"cosine\": get_cosine_scaled_reward(\n",
    "        #     min_value_wrong=script_args.cosine_min_value_wrong,\n",
    "        #     max_value_wrong=script_args.cosine_max_value_wrong,\n",
    "        #     min_value_correct=script_args.cosine_min_value_correct,\n",
    "        #     max_value_correct=script_args.cosine_max_value_correct,\n",
    "        #     max_len=script_args.cosine_max_len,\n",
    "        # ),\n",
    "        # \"repetition_penalty\": get_repetition_penalty_reward(\n",
    "        #     ngram_size=script_args.repetition_n_grams,\n",
    "        #     max_penalty=script_args.repetition_max_penalty,\n",
    "        # ),\n",
    "        # \"length\": len_reward,\n",
    "        # \"code\": update_wrapper(\n",
    "        #     partial(\n",
    "        #         code_reward,\n",
    "        #         num_parallel=script_args.parallel_code_exec_per_proc,\n",
    "        #         provider_type=script_args.code_provider,\n",
    "        #         enforce_same_language=getattr(script_args, \"enforce_same_language\", False),\n",
    "        #     ),\n",
    "        #     code_reward,\n",
    "        # ),\n",
    "        # \"binary_code\": update_wrapper(\n",
    "        #     partial(\n",
    "        #         binary_code_reward,\n",
    "        #         num_parallel=script_args.parallel_code_exec_per_proc,\n",
    "        #         provider_type=script_args.code_provider,\n",
    "        #         enforce_same_language=getattr(script_args, \"enforce_same_language\", False),\n",
    "        #     ),\n",
    "        #     binary_code_reward,\n",
    "        # ),\n",
    "        # \"ioi_code\": update_wrapper(\n",
    "        #     partial(\n",
    "        #         ioi_code_reward,\n",
    "        #         test_batch_size=script_args.code_eval_test_batch_size,\n",
    "        #         provider_type=getattr(script_args, \"ioi_provider\", \"piston\"),\n",
    "        #     ),\n",
    "        #     ioi_code_reward,\n",
    "        # ),\n",
    "        # \"cf_code\": update_wrapper(\n",
    "        #     partial(\n",
    "        #         cf_code_reward,\n",
    "        #         test_batch_size=script_args.code_eval_test_batch_size,\n",
    "        #         scoring_mode=script_args.code_eval_scoring_mode,\n",
    "        #     ),\n",
    "        #     cf_code_reward,\n",
    "        # ),\n",
    "        # \"code_format\": get_code_format_reward(language=script_args.code_language),\n",
    "        \"tag_count\": tag_count_reward,\n",
    "        # \"soft_overlong_punishment\": get_soft_overlong_punishment(\n",
    "        #     max_completion_len=script_args.max_completion_len,\n",
    "        #     soft_punish_cache=script_args.soft_punish_cache,\n",
    "        # ),\n",
    "    }\n",
    "    reward_funcs = [REWARD_FUNCS_REGISTRY[func] for func in script_args.reward_funcs]\n",
    "\n",
    "    return reward_funcs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4bd91",
   "metadata": {},
   "source": [
    "### è¨“ç·´è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7bcc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨å ±é…¬é–¢æ•°ã®è¨­å®š\n",
    "\n",
    "script_args = GRPOScriptArguments(\n",
    "    # https://huggingface.co/datasets/open-r1/OpenR1-Math-220k\n",
    "    dataset_name=\"open-r1/OpenR1-Math-220k\",\n",
    "    dataset_prompt_column=\"problem\",\n",
    "    reward_funcs=[\"accuracy\", \"format\", \"tag_count\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPOã®è¨“ç·´è¨­å®š\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    bf16=True,\n",
    "    use_vllm=True,\n",
    "    do_eval=False,\n",
    "    gradient_accumulation_steps=16, # 4ã‹ã‚‰16ã«å¢—åŠ ã€‚ ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’å›é¿ã™ã‚‹ãŸã‚\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    # hub_model_id=\"DeepSeek-R1-Distill-Qwen-1.5B-GRPO\",\n",
    "    hub_model_id=\"DeepSeek-R1-Distill-Qwen-0.6B-GRPO\",\n",
    "    hub_strategy=\"every_save\",\n",
    "    learning_rate=1.0e-6, # SFTã‚ˆã‚Šã‚‚ä½ã„å­¦ç¿’ç‡\n",
    "    log_completions=True,\n",
    "    log_level=\"info\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
    "    max_prompt_length=512,\n",
    "    max_completion_length=2048,\n",
    "    max_steps=1, # æœ¬æ¥ã¯-1 ã ãŒã€ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ç”¨ã«å°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«è¨­å®š\n",
    "    num_generations=4, # 16ã‹ã‚‰4ã«æ¸›å°‘ã€‚ ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’å›é¿ã™ã‚‹ãŸã‚\n",
    "    num_train_epochs=1, # 1ã‚¨ãƒãƒƒã‚¯ã®ã¿\n",
    "    # output_dir=\"data/DeepSeek-R1-Distill-Qwen-1.5B-GRPO\",\n",
    "    output_dir=\"data/DeepSeek-R1-Distill-Qwen-0.6B-GRPO\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=1, # æœ¬æ¥ã¯16ã ãŒã€ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’å›é¿ã™ã‚‹ãŸã‚1ã«è¨­å®š\n",
    "    per_device_train_batch_size=1, # æœ¬æ¥ã¯16ã ãŒã€ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’å›é¿ã™ã‚‹ãŸã‚1ã«è¨­å®š\n",
    "    push_to_hub=False, # æœ¬æ¥ã¯Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚True\n",
    "    report_to=[], # æœ¬æ¥ã¯ãƒ­ã‚°ã‚’ç›£è¦–ã™ã‚‹ãŸã‚[\"wandb\"]\n",
    "    reward_weights=[1.0, 1.0, 1.0], # å ±é…¬é–¢æ•°ã®é‡ã¿\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    temperature=0.7,\n",
    "    use_liger_kernel=True,\n",
    "    warmup_ratio=0.1,\n",
    "    system_prompt=\"You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\\n...\\n</think>\\n<answer>\\n...\\n</answer>\",\n",
    "    vllm_mode=\"colocate\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelConfig(\n",
    "    # SFTã§è¨“ç·´æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã™ã‚‹\n",
    "    # model_name_or_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    model_name_or_path=\"Qwen/Qwen3-0.6B-Base\",\n",
    "    model_revision=\"main\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a20d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´æ¸ˆã¿ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã€å†é–‹\n",
    "\n",
    "last_checkpoint = None\n",
    "\n",
    "if os.path.isdir(training_args.output_dir):\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\n",
    "if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "    logger.info(f\"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œå‡ºã€{last_checkpoint=} ã‹ã‚‰å­¦ç¿’ã‚’å†é–‹ã—ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afef070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & BiasesãŒæœ‰åŠ¹ãªå ´åˆã€åˆæœŸåŒ–\n",
    "\n",
    "if \"wandb\" in training_args.report_to:\n",
    "    init_wandb_training(training_args)\n",
    "    logger.info(\"Weights & Biasesã®åˆæœŸåŒ–ãŒå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905216e",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fec2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = get_dataset(script_args)\n",
    "dataset\n",
    "\n",
    "logger.info(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾— {dataset=}\")\n",
    "# 93733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8af0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒãƒƒã‚°ç”¨ã«1000ã‚µãƒ³ãƒ—ãƒ«ã«åˆ¶é™\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].select(range(1000))\n",
    "\n",
    "logger.info(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° {dataset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f78e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conversation(example, prompt_column: str = script_args.dataset_prompt_column):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å„ä¾‹ã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã—ã€promptãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½œæˆã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    prompt = []\n",
    "\n",
    "    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€æœ€åˆã«è¿½åŠ \n",
    "    if training_args.system_prompt is not None:\n",
    "        prompt.append({\"role\": \"system\", \"content\": training_args.system_prompt})\n",
    "\n",
    "    # problemã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ã‚¨ãƒ©ãƒ¼ã‚’ã‚¹ãƒ­ãƒ¼\n",
    "    if prompt_column not in example:\n",
    "        raise ValueError(f\"Dataset Question Field Error: {prompt_column} is not supported.\")\n",
    "\n",
    "    # problemã‚«ãƒ©ãƒ ã®å†…å®¹ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¿½åŠ \n",
    "    prompt.append({\"role\": \"user\", \"content\": example[prompt_column]})\n",
    "\n",
    "    # promptãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿”ã™\n",
    "    return {\"prompt\": prompt}\n",
    "\n",
    "dataset = dataset.map(make_conversation)\n",
    "logger.info(f\"ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾— {dataset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37080b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messagesã‚«ãƒ©ãƒ ã‚’å‰Šé™¤\n",
    "for split in dataset:\n",
    "    if \"messages\" in dataset[split].column_names:\n",
    "        dataset[split] = dataset[split].remove_columns(\"messages\")\n",
    "\n",
    "logger.info(f\"messagesã‚«ãƒ©ãƒ ã‚’å‰Šé™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾— {dataset=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad312e46",
   "metadata": {},
   "source": [
    "### ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22920fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "tokenizer = get_tokenizer(model_args, training_args)\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å–å¾— {tokenizer=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c469ad",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "model = get_model(model_args, training_args)\n",
    "logger.info(f\"ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾— {model=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69fe630",
   "metadata": {},
   "source": [
    "### å ±é…¬é–¢æ•°ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbccdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_funcs = get_reward_funcs(script_args)\n",
    "logger.info(f\"å ±é…¬é–¢æ•°ã‚’å–å¾— {reward_funcs=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7f0d9",
   "metadata": {},
   "source": [
    "### GRPOTrainerã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[script_args.dataset_train_split]\n",
    "logger.info(f\"{train_dataset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca13ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset  =(\n",
    "    dataset[script_args.dataset_test_split]\n",
    "    if training_args.eval_strategy != \"no\" else None\n",
    ")\n",
    "logger.info(f\"{eval_dataset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = get_peft_config(model_args)\n",
    "logger.info(f\"{peft_config=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d24d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(training_args, model_args)\n",
    "logger.info(f\"{callbacks=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ccd07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n",
      "ğŸŸ¦ Resetting dropped connection: stats.vllm.ai\n",
      "ğŸŸ¦ https://stats.vllm.ai:443 \"POST / HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=reward_funcs,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    callbacks=callbacks,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "\n",
    "elif last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "\n",
    "logger.info(f\"è¨“ç·´ã®å†é–‹ã«ä½¿ç”¨ã™ã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ {checkpoint=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8052db8e",
   "metadata": {},
   "source": [
    "### è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df515e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f1e35",
   "metadata": {},
   "source": [
    "### ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡º\n",
    "metrics = train_result.metrics\n",
    "\n",
    "# ã€Œå­¦ç¿’ã«ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ç·æ•°ã€ã‚’çµ±è¨ˆã«è¿½åŠ \n",
    "metrics[\"train_samples\"] = len(dataset[script_args.dataset_train_split])\n",
    "\n",
    "# ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "\n",
    "# JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "# Trainerã®å†…éƒ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆè¨­å®šã®EOSãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®EOSãƒˆãƒ¼ã‚¯ãƒ³IDã«è¨­å®š\n",
    "trainer.model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "trainer.save_model(training_args.output_dir)\n",
    "\n",
    "# ãã®ä»–ã®æƒ…å ±ã‚’ä¿å­˜\n",
    "if trainer.accelerator.is_main_process:\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚’ä½œæˆ\n",
    "    kwargs = {\n",
    "        \"dataset_name\": script_args.dataset_name,\n",
    "        \"tags\": [\"open-r1\"],\n",
    "    }\n",
    "    trainer.create_model_card(**kwargs)\n",
    "\n",
    "    # æ¨è«–ã®ãŸã‚ã«KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ–ã—ã€é«˜é€ŸåŒ–\n",
    "    trainer.model.config.use_cache = True\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’ä¿å­˜ï¼ˆconfig.jsonï¼‰\n",
    "    trainer.model.config.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©•ä¾¡ã®å®Ÿè¡Œ\n",
    "\n",
    "if training_args.do_eval:\n",
    "    metrics = trainer.evaluate()\n",
    "    metrics[\"eval_samples\"] = len(dataset[script_args.dataset_test_split])\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff39f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hubã«ãƒ—ãƒƒã‚·ãƒ¥\n",
    "\n",
    "if training_args.push_to_hub:\n",
    "    logger.info(\"Pushing to hub...\")\n",
    "    trainer.push_to_hub(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
