{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db3b17e",
   "metadata": {},
   "source": [
    "# Open-R1-Distill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c646831",
   "metadata": {},
   "source": [
    "- [huggingface/open-r1][2]\n",
    "- [open-r1/Mixture-of-Thoughts][1]\n",
    "\n",
    "[1]: https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts\n",
    "[2]: https://github.com/huggingface/open-r1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866f5d6",
   "metadata": {},
   "source": [
    "## Áí∞Â¢ÉÊßãÁØâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25414081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"üü¶\"\n",
    "        case logging.INFO:\n",
    "            level = \"üü©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"üü®\"\n",
    "        case logging.ERROR:\n",
    "            level = \"üü•\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"üõë\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "NVIDIA_SMI = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout\n",
    "logging.info(NVIDIA_SMI)\n",
    "logging.info(f\"Python {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce29f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Áí∞Â¢ÉÂ§âÊï∞„ÅÆË®≠ÂÆö\n",
    "# VLLM_USE_V1=0 „ÇíË®≠ÂÆö\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40811ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    if not os.path.exists(\"/content/open-r1\"):\n",
    "        %git clone https://github.com/huggingface/open-r1.git\n",
    "    %cd /content/open-r1\n",
    "    %pip install -e \".[dev]\" --no-deps\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    !apt update && apt install git-lfs -y\n",
    "    if not os.path.exists(\"/workspaces/open-r1-distill/open-r1\"):\n",
    "        %git clone https://github.com/huggingface/open-r1.git\n",
    "    %cd /workspaces/open-r1-distill/open-r1\n",
    "    %pip install -e \".[dev]\" --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca732ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) PyTorch„Å®Transformers„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "%pip install torch==2.6.0 transformers==4.52.3\n",
    "\n",
    "# 2) vLLM„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "%pip install vllm==0.8.5.post1\n",
    "\n",
    "# 3) Flash Attention„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "# 2.8.3„ÅØundefined symbol„Ç®„É©„Éº„ÅåÁô∫Áîü„Åô„Çã„Åü„ÇÅ2.7.3„Çí„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "# https://github.com/Dao-AILab/flash-attention/issues/1832\n",
    "%pip install \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl\" --no-build-isolation\n",
    "\n",
    "# 4) „Åù„ÅÆ‰ªñ„ÅÆÂøÖË¶Å„Å™„Éë„ÉÉ„Ç±„Éº„Ç∏„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "%pip install \\\n",
    "    accelerate==1.4.0 \\\n",
    "    async-lru \\\n",
    "    bitsandbytes \\\n",
    "    \"distilabel[vllm]\" \\\n",
    "    deepspeed==0.16.8 \\\n",
    "    hf_transfer \\\n",
    "    langdetect \\\n",
    "    latex2sympy2_extended \\\n",
    "    liger-kernel \\\n",
    "    \"trl[vllm]==0.18.0\" \\\n",
    "    math-verify==0.5.2 \\\n",
    "    wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f16ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c803c6",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if IN_COLAB:\n",
    "#     %cd /content/open-r1\n",
    "# else:\n",
    "#     %cd /workspaces/open-r1-distill/open-r1\n",
    "\n",
    "# !python pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from distilabel.models import vLLM\n",
    "from distilabel.pipeline import Pipeline\n",
    "from distilabel.steps.tasks import TextGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Çπ„ÉÜ„ÉÉ„Éó„Éê„Ç§„Çπ„ÉÜ„ÉÉ„Éó„ÅßÊé®Ë´ñ„Åó„ÄÅÊúÄÁµÇÂõûÁ≠î„Çí\\boxed{}„ÅßÂõ≤„Å£„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "prompt_template = \"\"\"\\\n",
    "You will be given a problem. Please reason step by step, and put your final answer within \\boxed{}:\n",
    "{{ instruction }}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c234e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êï∞Â≠¶„Éá„Éº„Çø„Çª„ÉÉ„Éà\n",
    "# https://huggingface.co/datasets/AI-MO/NuminaMath-TIR\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"AI-MO/NuminaMath-TIR\",\n",
    "    split=\"train\",\n",
    ").select(range(10))\n",
    "\n",
    "len(dataset), dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99118f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][\"problem\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][\"solution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26adf858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e58186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange with another smol distilled r1\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÂÆöÁæ©„Å®LLM„ÅÆË®≠ÂÆö\n",
    "\n",
    "with Pipeline(\n",
    "    name=\"distill-qwen-7b-r1\",\n",
    "    description=\"A pipeline to generate data from a distilled r1 model\",\n",
    ") as pipeline:\n",
    "\n",
    "    # distilabel„ÅÆvLLM„ÇíÂàùÊúüÂåñ\n",
    "    # https://distilabel.argilla.io/dev/components-gallery/llms/vllm/?h=vllm\n",
    "    llm = vLLM(\n",
    "        model=model_id,\n",
    "        tokenizer=model_id,\n",
    "        extra_kwargs={\n",
    "            \"tensor_parallel_size\": 1,\n",
    "            # \"max_model_len\": 8192,\n",
    "            \"max_model_len\": 1024,\n",
    "        },\n",
    "        generation_kwargs={\n",
    "            \"temperature\": 0.6,\n",
    "            # \"max_new_tokens\": 8192,\n",
    "            \"max_new_tokens\": 1024,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    prompt_column = \"problem\"\n",
    "\n",
    "    text_generation = TextGeneration(\n",
    "        llm=llm, \n",
    "        template=prompt_template,\n",
    "        num_generations=4,\n",
    "        input_mappings={\"instruction\": prompt_column} if prompt_column is not None else {}\n",
    "    )\n",
    "\n",
    "distiset = pipeline.run(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distiset.save_to_disk(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distilabel.distiset import Distiset\n",
    "\n",
    "ds = Distiset.load_from_disk(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0634573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distiset[\"default\"][\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distiset[\"default\"][\"train\"][0][\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425af5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distiset[\"default\"][\"train\"][0][\"distilabel_metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distiset[\"default\"][\"train\"][0][\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d4779",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78308224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python src/open_r1/sft.py \\\n",
    "#     --config recipes/OpenR1-Distill-7B/sft/config_distill.yaml \\\n",
    "#     --model_name_or_path Qwen/Qwen3-0.6B-Base \\\n",
    "#     --hub_model_id OpenR1-Distill-0.6B \\\n",
    "#     --output_dir data/OpenR1-Distill-0.6B \\\n",
    "#     --push_to_hub False \\\n",
    "#     --report_to none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import set_seed\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from open_r1.configs import ScriptArguments, SFTConfig\n",
    "from open_r1.utils import get_dataset, get_model, get_tokenizer\n",
    "from open_r1.utils.callbacks import get_callbacks\n",
    "from open_r1.utils.wandb_logging import init_wandb_training\n",
    "from trl import ModelConfig, SFTTrainer, TrlParser, get_peft_config, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path: open-r1/Qwen2.5-Math-7B-RoPE-300k\n",
    "# model_revision: main\n",
    "# torch_dtype: bfloat16\n",
    "# attn_implementation: flash_attention_2\n",
    "\n",
    "model_args = ModelConfig(\n",
    "    # model_name_or_path=\"open-r1/Qwen2.5-Math-7B-RoPE-300k\",\n",
    "    model_name_or_path=\"Qwen/Qwen3-0.6B-Base\",\n",
    "    model_revision=\"main\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    bf16=True,\n",
    "    do_eval=False,\n",
    "    eval_strategy=\"no\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    hub_model_id=\"OpenR1-Distill-0.6B\", # \"OpenR1-Distill-7B\"\n",
    "    hub_strategy=\"every_save\",\n",
    "    learning_rate=4.0e-5,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
    "    packing=False,\n",
    "    max_grad_norm=0.2,\n",
    "    max_length=32768,\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1, # 5\n",
    "    output_dir=\"data/OpenR1-Distill-0.6B\", # \"data/OpenR1-Distill-7B\"\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    push_to_hub=False, # True\n",
    "    report_to=[], # [\"wandb\"]\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    use_liger_kernel=True,\n",
    "    warmup_ratio=0.03,\n",
    "    dataset_num_proc=12,\n",
    "    eos_token=\"<|im_end|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1361e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name: open-r1/Mixture-of-Thoughts\n",
    "# dataset_config: all\n",
    "# dataset_num_proc: 12\n",
    "# eos_token: <|im_end|>\n",
    "\n",
    "script_args = ScriptArguments(\n",
    "    dataset_name=\"open-r1/Mixture-of-Thoughts\",\n",
    "    dataset_config=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Setup logging\n",
    "###############\n",
    "# logging.basicConfig(\n",
    "#     format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "#     datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "#     handlers=[logging.StreamHandler(sys.stdout)],\n",
    "# )\n",
    "# log_level = training_args.get_process_log_level()\n",
    "# logger.setLevel(log_level)\n",
    "# datasets.utils.logging.set_verbosity(log_level)\n",
    "# transformers.utils.logging.set_verbosity(log_level)\n",
    "# transformers.utils.logging.enable_default_handler()\n",
    "# transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# logger.info(f\"Model parameters {model_args}\")\n",
    "# logger.info(f\"Script parameters {script_args}\")\n",
    "# logger.info(f\"Training parameters {training_args}\")\n",
    "\n",
    "# Check for last checkpoint\n",
    "last_checkpoint = None\n",
    "\n",
    "if os.path.isdir(training_args.output_dir):\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\n",
    "if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "    logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint=}.\")\n",
    "\n",
    "if \"wandb\" in training_args.report_to:\n",
    "    init_wandb_training(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88323001",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Load dataset, tokenizer, and model #\n",
    "######################################\n",
    "dataset = get_dataset(script_args)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Çµ„Éñ„Çµ„É≥„Éó„É™„É≥„Ç∞\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].select(range(1000))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb95957",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(model_args, training_args)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.chat_template is None:\n",
    "    logger.info(\"No chat template provided, defaulting to ChatML.\")\n",
    "    model, tokenizer = setup_chat_format(model, tokenizer, format=\"chatml\")\n",
    "\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(model_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Initialize the SFT Trainer\n",
    "############################\n",
    "\n",
    "training_args.max_steps = 10\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[script_args.dataset_train_split],\n",
    "    eval_dataset=(dataset[script_args.dataset_test_split] if training_args.eval_strategy != \"no\" else None),\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=get_peft_config(model_args),\n",
    "    callbacks=get_callbacks(training_args, model_args),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48698aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Training loop\n",
    "###############\n",
    "checkpoint = None\n",
    "\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "elif last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75af911",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "metrics[\"train_samples\"] = len(dataset[script_args.dataset_train_split])\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b819cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer\n",
    "del model\n",
    "del dataset\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc28e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996b621",
   "metadata": {},
   "source": [
    "## GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c88de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if IN_COLAB:\n",
    "#     %cd /content/open-r1\n",
    "# else:\n",
    "#     %cd /workspaces/open-r1-distill/open-r1\n",
    "\n",
    "# !RANK=0 WORLD_SIZE=1 LOCAL_RANK=0 MASTER_ADDR=localhost MASTER_PORT=12345 python src/open_r1/grpo.py \\\n",
    "#     --config recipes/DeepSeek-R1-Distill-Qwen-1.5B/grpo/config_demo.yaml \\\n",
    "#     --model_name_or_path Qwen/Qwen3-0.6B-Base \\\n",
    "#     --hub_model_id OpenR1-Distill-0.6B \\\n",
    "#     --output_dir data/OpenR1-Distill-0.6B \\\n",
    "#     --push_to_hub False \\\n",
    "#     --report_to none \\\n",
    "#     --vllm_mode colocate \\\n",
    "#     --per_device_train_batch_size 1 \\\n",
    "#     --num_generations 4 \\\n",
    "#     --gradient_accumulation_steps 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f204686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import set_seed\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from open_r1.configs import GRPOConfig, GRPOScriptArguments\n",
    "from open_r1.rewards import get_reward_funcs\n",
    "from open_r1.utils import get_dataset, get_model, get_tokenizer\n",
    "from open_r1.utils.callbacks import get_callbacks\n",
    "from open_r1.utils.wandb_logging import init_wandb_training\n",
    "from trl import GRPOTrainer, ModelConfig, TrlParser, get_peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e763f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# !RANK=0 WORLD_SIZE=1 LOCAL_RANK=0 MASTER_ADDR=localhost MASTER_PORT=12345 \n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12345\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7bcc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_args = GRPOScriptArguments(\n",
    "    dataset_name=\"open-r1/OpenR1-Math-220k\",\n",
    "    dataset_prompt_column=\"problem\",\n",
    "    reward_funcs=[\"accuracy\", \"format\", \"tag_count\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    bf16=True,\n",
    "    use_vllm=True,\n",
    "    do_eval=False,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    # hub_model_id=\"DeepSeek-R1-Distill-Qwen-1.5B-GRPO\",\n",
    "    hub_model_id=\"DeepSeek-R1-Distill-Qwen-0.6B-GRPO\",\n",
    "    hub_strategy=\"every_save\",\n",
    "    learning_rate=1.0e-6,\n",
    "    log_completions=True,\n",
    "    log_level=\"info\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
    "    max_prompt_length=512,\n",
    "    max_completion_length=2048,\n",
    "    max_steps=-1,\n",
    "    # num_generations=16,\n",
    "    num_generations=4,\n",
    "    num_train_epochs=1,\n",
    "    # output_dir=\"data/DeepSeek-R1-Distill-Qwen-1.5B-GRPO\",\n",
    "    output_dir=\"data/DeepSeek-R1-Distill-Qwen-0.6B-GRPO\",\n",
    "    overwrite_output_dir=True,\n",
    "    # per_device_eval_batch_size=16,\n",
    "    per_device_eval_batch_size=1,\n",
    "    # per_device_train_batch_size=16,\n",
    "    per_device_train_batch_size=1,\n",
    "    # push_to_hub=True,\n",
    "    push_to_hub=False,\n",
    "    # report_to=[\"wandb\"],\n",
    "    report_to=[],\n",
    "    # reward_funcs=[\"accuracy\", \"format\", \"tag_count\"],\n",
    "    reward_weights=[1.0, 1.0, 1.0],\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    temperature=0.7,\n",
    "    use_liger_kernel=True,\n",
    "    warmup_ratio=0.1,\n",
    "    system_prompt=\"You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\\n...\\n</think>\\n<answer>\\n...\\n</answer>\",\n",
    "    # chat_template=\"\",\n",
    "    vllm_mode=\"colocate\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "# model_revision: main\n",
    "# torch_dtype: bfloat16\n",
    "# attn_implementation: flash_attention_2\n",
    "\n",
    "model_args = ModelConfig(\n",
    "    # model_name_or_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    model_name_or_path=\"Qwen/Qwen3-0.6B-Base\",\n",
    "    model_revision=\"main\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Setup logging\n",
    "###############\n",
    "# logging.basicConfig(\n",
    "#     format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "#     datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "#     handlers=[logging.StreamHandler(sys.stdout)],\n",
    "# )\n",
    "# log_level = training_args.get_process_log_level()\n",
    "# logger.setLevel(log_level)\n",
    "# datasets.utils.logging.set_verbosity(log_level)\n",
    "# transformers.utils.logging.set_verbosity(log_level)\n",
    "# transformers.utils.logging.enable_default_handler()\n",
    "# transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# # Log on each process a small summary\n",
    "# logger.warning(\n",
    "#     f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "#     + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "# )\n",
    "# logger.info(f\"Model parameters {model_args}\")\n",
    "# logger.info(f\"Script parameters {script_args}\")\n",
    "# logger.info(f\"Training parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a20d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for last checkpoint\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir):\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "    logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint=}.\")\n",
    "\n",
    "if \"wandb\" in training_args.report_to:\n",
    "    init_wandb_training(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fec2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = get_dataset(script_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d508b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8af0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].select(range(1000))  # „Éá„Éê„ÉÉ„Ç∞Áî®„Å´1000„Çµ„É≥„Éó„É´„Å´Âà∂Èôê\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22920fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Load tokenizer\n",
    "################\n",
    "tokenizer = get_tokenizer(model_args, training_args)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(model_args, training_args)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbccdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reward functions from the registry\n",
    "reward_funcs = get_reward_funcs(script_args)\n",
    "reward_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format into conversation\n",
    "def make_conversation(example, prompt_column: str = script_args.dataset_prompt_column):\n",
    "    prompt = []\n",
    "\n",
    "    if training_args.system_prompt is not None:\n",
    "        prompt.append({\"role\": \"system\", \"content\": training_args.system_prompt})\n",
    "\n",
    "    if prompt_column not in example:\n",
    "        raise ValueError(f\"Dataset Question Field Error: {prompt_column} is not supported.\")\n",
    "\n",
    "    prompt.append({\"role\": \"user\", \"content\": example[prompt_column]})\n",
    "    return {\"prompt\": prompt}\n",
    "\n",
    "dataset = dataset.map(make_conversation)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dbdb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset:\n",
    "    if \"messages\" in dataset[split].column_names:\n",
    "        dataset[split] = dataset[split].remove_columns(\"messages\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ccd07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Initialize the GRPO trainer\n",
    "#############################\n",
    "training_args.max_steps = 10\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=reward_funcs,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[script_args.dataset_train_split],\n",
    "    eval_dataset=(dataset[script_args.dataset_test_split] if training_args.eval_strategy != \"no\" else None),\n",
    "    peft_config=get_peft_config(model_args),\n",
    "    callbacks=get_callbacks(training_args, model_args),\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "elif last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df515e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "metrics = train_result.metrics\n",
    "metrics[\"train_samples\"] = len(dataset[script_args.dataset_train_split])\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Save model and create model card\n",
    "##################################\n",
    "# Align the model's generation config with the tokenizer's eos token\n",
    "# to avoid unbounded generation in the transformers `pipeline()` function\n",
    "trainer.model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "trainer.save_model(training_args.output_dir)\n",
    "\n",
    "# Save everything else on main process\n",
    "kwargs = {\n",
    "    \"dataset_name\": script_args.dataset_name,\n",
    "    \"tags\": [\"open-r1\"],\n",
    "}\n",
    "if trainer.accelerator.is_main_process:\n",
    "    trainer.create_model_card(**kwargs)\n",
    "    # Restore k,v cache for fast inference\n",
    "    trainer.model.config.use_cache = True\n",
    "    trainer.model.config.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Evaluate\n",
    "##########\n",
    "if training_args.do_eval:\n",
    "    metrics = trainer.evaluate()\n",
    "    metrics[\"eval_samples\"] = len(dataset[script_args.dataset_test_split])\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff39f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# push to hub\n",
    "#############\n",
    "if training_args.push_to_hub:\n",
    "    logger.info(\"Pushing to hub...\")\n",
    "    trainer.push_to_hub(**kwargs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
